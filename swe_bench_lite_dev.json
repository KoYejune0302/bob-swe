[
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-1625",
        "base_commit": "14e1a23a3166b9a645a16de96f694c77a5d4abb7",
        "problem_statement": "TSQL - L031 incorrectly triggers \"Avoid using aliases in join condition\" when no join present\n## Expected Behaviour\r\n\r\nBoth of these queries should pass, the only difference is the addition of a table alias 'a':\r\n\r\n1/ no alias\r\n\r\n```\r\nSELECT [hello]\r\nFROM\r\n    mytable\r\n```\r\n\r\n2/ same query with alias\r\n\r\n```\r\nSELECT a.[hello]\r\nFROM\r\n    mytable AS a\r\n```\r\n\r\n## Observed Behaviour\r\n\r\n1/ passes\r\n2/ fails with: L031: Avoid using aliases in join condition.\r\n\r\nBut there is no join condition :-)\r\n\r\n## Steps to Reproduce\r\n\r\nLint queries above\r\n\r\n## Dialect\r\n\r\nTSQL\r\n\r\n## Version\r\n\r\nsqlfluff 0.6.9\r\nPython 3.6.9\r\n\r\n## Configuration\r\n\r\nN/A\n"
    },
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-2419",
        "base_commit": "f1dba0e1dd764ae72d67c3d5e1471cf14d3db030",
        "problem_statement": "Rule L060 could give a specific error message\nAt the moment rule L060 flags something like this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL' or 'NVL'.\r\n```\r\n\r\nSince we likely know the wrong word, it might be nice to actually flag that instead of both `IFNULL` and `NVL` - like most of the other rules do.\r\n\r\nThat is it should flag this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'IFNULL'.\r\n```\r\n Or this:\r\n\r\n```\r\nL:  21 | P:   9 | L060 | Use 'COALESCE' instead of 'NVL'.\r\n```\r\n\r\nAs appropriate.\r\n\r\nWhat do you think @jpy-git ?\r\n\n"
    },
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-1733",
        "base_commit": "a1579a16b1d8913d9d7c7d12add374a290bcc78c",
        "problem_statement": "Extra space when first field moved to new line in a WITH statement\nNote, the query below uses a `WITH` statement. If I just try to fix the SQL within the CTE, this works fine.\r\n\r\nGiven the following SQL:\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Expected Behaviour\r\n\r\nafter running `sqlfluff fix` I'd expect (`my_id` gets moved down and indented properly):\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n        my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Observed Behaviour\r\n\r\nafter running `sqlfluff fix` we get (notice that `my_id` is indented one extra space)\r\n\r\n```sql\r\nWITH example AS (\r\n    SELECT\r\n         my_id,\r\n        other_thing,\r\n        one_more\r\n    FROM\r\n        my_table\r\n)\r\n\r\nSELECT *\r\nFROM example\r\n```\r\n\r\n## Steps to Reproduce\r\n\r\nNoted above. Create a file with the initial SQL and fun `sqfluff fix` on it.\r\n\r\n## Dialect\r\n\r\nRunning with default config.\r\n\r\n## Version\r\nInclude the output of `sqlfluff --version` along with your Python version\r\n\r\nsqlfluff, version 0.7.0\r\nPython 3.7.5\r\n\r\n## Configuration\r\n\r\nDefault config.\r\n\n"
    },
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-1517",
        "base_commit": "304a197829f98e7425a46d872ada73176137e5ae",
        "problem_statement": "\"Dropped elements in sequence matching\" when doubled semicolon\n## Expected Behaviour\r\nFrankly, I'm not sure whether it (doubled `;`) should be just ignored or rather some specific rule should be triggered.\r\n## Observed Behaviour\r\n```console\r\n(.venv) ?master ~/prod/_inne/sqlfluff> echo \"select id from tbl;;\" | sqlfluff lint -\r\nTraceback (most recent call last):\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/bin/sqlfluff\", line 11, in <module>\r\n    load_entry_point('sqlfluff', 'console_scripts', 'sqlfluff')()\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1137, in __call__\r\n    return self.main(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1062, in main\r\n    rv = self.invoke(ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1668, in invoke\r\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\r\n    return ctx.invoke(self.callback, **ctx.params)\r\n  File \"/home/adam/prod/_inne/sqlfluff/.venv/lib/python3.9/site-packages/click/core.py\", line 763, in invoke\r\n    return __callback(*args, **kwargs)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/cli/commands.py\", line 347, in lint\r\n    result = lnt.lint_string_wrapped(sys.stdin.read(), fname=\"stdin\")\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 789, in lint_string_wrapped\r\n    linted_path.add(self.lint_string(string, fname=fname, fix=fix))\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 668, in lint_string\r\n    parsed = self.parse_string(in_str=in_str, fname=fname, config=config)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 607, in parse_string\r\n    return self.parse_rendered(rendered, recurse=recurse)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 313, in parse_rendered\r\n    parsed, pvs = cls._parse_tokens(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/linter/linter.py\", line 190, in _parse_tokens\r\n    parsed: Optional[BaseSegment] = parser.parse(\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/parser.py\", line 32, in parse\r\n    parsed = root_segment.parse(parse_context=ctx)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/segments/base.py\", line 821, in parse\r\n    check_still_complete(segments, m.matched_segments, m.unmatched_segments)\r\n  File \"/home/adam/prod/_inne/sqlfluff/src/sqlfluff/core/parser/helpers.py\", line 30, in check_still_complete\r\n    raise RuntimeError(\r\nRuntimeError: Dropped elements in sequence matching! 'select id from tbl;;' != ';'\r\n\r\n```\r\n## Steps to Reproduce\r\nRun \r\n```console\r\necho \"select id from tbl;;\" | sqlfluff lint -\r\n```\r\n## Dialect\r\ndefault (ansi)\r\n## Version\r\n```\r\nsqlfluff, version 0.6.6\r\nPython 3.9.5\r\n```\r\n## Configuration\r\nNone\r\n\n"
    },
    {
        "repo": "sqlfluff/sqlfluff",
        "instance_id": "sqlfluff__sqlfluff-1763",
        "base_commit": "a10057635e5b2559293a676486f0b730981f037a",
        "problem_statement": "dbt postgres fix command errors with UnicodeEncodeError and also wipes the .sql file\n_If this is a parsing or linting issue, please include a minimal SQL example which reproduces the issue, along with the `sqlfluff parse` output, `sqlfluff lint` output and `sqlfluff fix` output when relevant._\r\n\r\n## Expected Behaviour\r\nViolation failure notice at a minimum, without wiping the file. Would like a way to ignore the known error at a minimum as --noqa is not getting past this. Actually would expect --noqa to totally ignore this.\r\n\r\n## Observed Behaviour\r\nReported error: `UnicodeEncodeError: 'charmap' codec can't encode character '\\u2192' in position 120: character maps to <undefined>`\r\n\r\n## Steps to Reproduce\r\nSQL file:\r\n```sql\r\nSELECT\r\n    reacted_table_name_right.descendant_id AS category_id,\r\n    string_agg(redacted_table_name_left.name, ' \u2192 ' ORDER BY reacted_table_name_right.generations DESC) AS breadcrumbs -- noqa\r\nFROM {{ ref2('redacted_schema_name', 'redacted_table_name_left') }} AS redacted_table_name_left\r\nINNER JOIN {{ ref2('redacted_schema_name', 'reacted_table_name_right') }} AS reacted_table_name_right\r\n    ON redacted_table_name_left.id = order_issue_category_hierarchies.ancestor_id\r\nGROUP BY reacted_table_name_right.descendant_id\r\n```\r\nRunning `sqlfluff fix --ignore templating,parsing,lexing -vvvv` and accepting proposed fixes for linting violations.\r\n\r\n## Dialect\r\n`postgres`, with `dbt` templater\r\n\r\n## Version\r\n`python 3.7.12`\r\n`sqlfluff 0.7.0`\r\n`sqlfluff-templater-dbt 0.7.0`\r\n\r\n## Configuration\r\nI've tried a few, here's one:\r\n```\r\n[sqlfluff]\r\nverbose = 2\r\ndialect = postgres\r\ntemplater = dbt\r\nexclude_rules = None\r\noutput_line_length = 80\r\nrunaway_limit = 10\r\nignore_templated_areas = True\r\nprocesses = 3\r\n# Comma separated list of file extensions to lint.\r\n\r\n# NB: This config will only apply in the root folder.\r\nsql_file_exts = .sql\r\n\r\n[sqlfluff:indentation]\r\nindented_joins = False\r\nindented_using_on = True\r\ntemplate_blocks_indent = True\r\n\r\n[sqlfluff:templater]\r\nunwrap_wrapped_queries = True\r\n\r\n[sqlfluff:templater:jinja]\r\napply_dbt_builtins = True\r\n\r\n[sqlfluff:templater:jinja:macros]\r\n# Macros provided as builtins for dbt projects\r\ndbt_ref = {% macro ref(model_ref) %}{{model_ref}}{% endmacro %}\r\ndbt_source = {% macro source(source_name, table) %}{{source_name}}_{{table}}{% endmacro %}\r\ndbt_config = {% macro config() %}{% for k in kwargs %}{% endfor %}{% endmacro %}\r\ndbt_var = {% macro var(variable, default='') %}item{% endmacro %}\r\ndbt_is_incremental = {% macro is_incremental() %}True{% endmacro %}\r\n\r\n# Common config across rules\r\n[sqlfluff:rules]\r\ntab_space_size = 4\r\nindent_unit = space\r\nsingle_table_references = consistent\r\nunquoted_identifiers_policy = all\r\n\r\n# L001 - Remove trailing whitespace (fix)\r\n# L002 - Single section of whitespace should not contain both tabs and spaces (fix)\r\n# L003 - Keep consistent indentation (fix)\r\n# L004 - We use 4 spaces for indentation just for completeness (fix)\r\n# L005 - Remove space before commas (fix)\r\n# L006 - Operators (+, -, *, /) will be wrapped by a single space each side (fix)\r\n\r\n# L007 - Operators should not be at the end of a line\r\n[sqlfluff:rules:L007]  # Keywords\r\noperator_new_lines = after\r\n\r\n# L008 - Always use a single whitespace after a comma (fix)\r\n# L009 - Files will always end with a trailing newline\r\n\r\n# L010 - All keywords will use full upper case (fix)\r\n[sqlfluff:rules:L010]  # Keywords\r\ncapitalisation_policy = upper\r\n\r\n# L011 - Always explicitly alias tables (fix)\r\n[sqlfluff:rules:L011]  # Aliasing\r\naliasing = explicit\r\n\r\n# L012 - Do not have to explicitly alias all columns\r\n[sqlfluff:rules:L012]  # Aliasing\r\naliasing = explicit\r\n\r\n# L013 - Always explicitly alias a column with an expression in it (fix)\r\n[sqlfluff:rules:L013]  # Aliasing\r\nallow_scalar = False\r\n\r\n# L014 - Always user full lower case for 'quoted identifiers' -> column refs. without an alias (fix)\r\n[sqlfluff:rules:L014]  # Unquoted identifiers\r\nextended_capitalisation_policy = lower\r\n\r\n# L015 - Always remove parenthesis when using DISTINCT to be clear that DISTINCT applies to all columns (fix)\r\n\r\n# L016 - Lines should be 120 characters of less. Comment lines should not be ignored (fix)\r\n[sqlfluff:rules:L016]\r\nignore_comment_lines = False\r\nmax_line_length = 120\r\n\r\n# L017 - There should not be whitespace between function name and brackets (fix)\r\n# L018 - Always align closing bracket of WITH to the WITH keyword (fix)\r\n\r\n# L019 - Always use trailing commas / commas at the end of the line (fix)\r\n[sqlfluff:rules:L019]\r\ncomma_style = trailing\r\n\r\n# L020 - Table aliases will always be unique per statement\r\n# L021 - Remove any use of ambiguous DISTINCT and GROUP BY combinations. Lean on removing the GROUP BY.\r\n# L022 - Add blank lines after common table expressions (CTE) / WITH.\r\n# L023 - Always add a single whitespace after AS in a WITH clause (fix)\r\n\r\n[sqlfluff:rules:L026]\r\nforce_enable = False\r\n\r\n# L027 - Always add references if more than one referenced table or view is used\r\n\r\n[sqlfluff:rules:L028]\r\nforce_enable = False\r\n\r\n[sqlfluff:rules:L029]  # Keyword identifiers\r\nunquoted_identifiers_policy = aliases\r\n\r\n[sqlfluff:rules:L030]  # Function names\r\ncapitalisation_policy = upper\r\n\r\n# L032 - We prefer use of join keys rather than USING\r\n# L034 - We prefer ordering of columns in select statements as (fix):\r\n# 1. wildcards\r\n# 2. single identifiers\r\n# 3. calculations and aggregates\r\n\r\n# L035 - Omit 'else NULL'; it is redundant (fix)\r\n# L036 - Move select targets / identifiers onto new lines each (fix)\r\n# L037 - When using ORDER BY, make the direction explicit (fix)\r\n\r\n# L038 - Never use trailing commas at the end of the SELECT clause\r\n[sqlfluff:rules:L038]\r\nselect_clause_trailing_comma = forbid\r\n\r\n# L039 - Remove unnecessary whitespace (fix)\r\n\r\n[sqlfluff:rules:L040]  # Null & Boolean Literals\r\ncapitalisation_policy = upper\r\n\r\n# L042 - Join clauses should not contain subqueries. Use common tables expressions (CTE) instead.\r\n[sqlfluff:rules:L042]\r\n# By default, allow subqueries in from clauses, but not join clauses.\r\nforbid_subquery_in = join\r\n\r\n# L043 - Reduce CASE WHEN conditions to COALESCE (fix)\r\n# L044 - Prefer a known number of columns along the path to the source data\r\n# L045 - Remove unused common tables expressions (CTE) / WITH statements (fix)\r\n# L046 - Jinja tags should have a single whitespace on both sides\r\n\r\n# L047 - Use COUNT(*) instead of COUNT(0) or COUNT(1) alternatives (fix)\r\n[sqlfluff:rules:L047]  # Consistent syntax to count all rows\r\nprefer_count_1 = False\r\nprefer_count_0 = False\r\n\r\n# L048 - Quoted literals should be surrounded by a single whitespace (fix)\r\n# L049 - Always use IS or IS NOT for comparisons with NULL (fix)\r\n```\r\n\n"
    },
    {
        "repo": "marshmallow-code/marshmallow",
        "instance_id": "marshmallow-code__marshmallow-1359",
        "base_commit": "b40a0f4e33823e6d0f341f7e8684e359a99060d1",
        "problem_statement": "3.0: DateTime fields cannot be used as inner field for List or Tuple fields\nBetween releases 3.0.0rc8 and 3.0.0rc9, `DateTime` fields have started throwing an error when being instantiated as inner fields of container fields like `List` or `Tuple`. The snippet below works in <=3.0.0rc8 and throws the error below in >=3.0.0rc9 (and, worryingly, 3.0.0):\r\n\r\n```python\r\nfrom marshmallow import fields, Schema\r\n\r\nclass MySchema(Schema):\r\n    times = fields.List(fields.DateTime())\r\n\r\ns = MySchema()\r\n```\r\n\r\nTraceback:\r\n```\r\nTraceback (most recent call last):\r\n  File \"test-mm.py\", line 8, in <module>\r\n    s = MySchema()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 383, in __init__\r\n    self.fields = self._init_fields()\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 913, in _init_fields\r\n    self._bind_field(field_name, field_obj)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/schema.py\", line 969, in _bind_field\r\n    field_obj._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 636, in _bind_to_schema\r\n    self.inner._bind_to_schema(field_name, self)\r\n  File \"/Users/victor/.pyenv/versions/marshmallow/lib/python3.6/site-packages/marshmallow/fields.py\", line 1117, in _bind_to_schema\r\n    or getattr(schema.opts, self.SCHEMA_OPTS_VAR_NAME)\r\nAttributeError: 'List' object has no attribute 'opts'\r\n```\r\n\r\nIt seems like it's treating the parent field as a Schema without checking that it is indeed a schema, so the `schema.opts` statement fails as fields don't have an `opts` attribute.\n"
    },
    {
        "repo": "marshmallow-code/marshmallow",
        "instance_id": "marshmallow-code__marshmallow-1343",
        "base_commit": "2be2d83a1a9a6d3d9b85804f3ab545cecc409bb0",
        "problem_statement": "[version 2.20.0] TypeError: 'NoneType' object is not subscriptable\nAfter update from version 2.19.5 to 2.20.0 I got error for code like:\r\n\r\n```python\r\nfrom marshmallow import Schema, fields, validates\r\n\r\n\r\nclass Bar(Schema):\r\n    value = fields.String()\r\n\r\n    @validates('value')  # <- issue here\r\n    def validate_value(self, value):\r\n        pass\r\n\r\n\r\nclass Foo(Schema):\r\n    bar = fields.Nested(Bar)\r\n\r\n\r\nsch = Foo()\r\n\r\nsch.validate({\r\n    'bar': 'invalid',\r\n})\r\n```\r\n\r\n```\r\nTraceback (most recent call last):\r\n  File \"/_/bug_mschema.py\", line 19, in <module>\r\n    'bar': 'invalid',\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 628, in validate\r\n    _, errors = self._do_load(data, many, partial=partial, postprocess=False)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 670, in _do_load\r\n    index_errors=self.opts.index_errors,\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 292, in deserialize\r\n    index=(index if index_errors else None)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 65, in call_and_store\r\n    value = getter_func(data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/marshalling.py\", line 285, in <lambda>\r\n    data\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 265, in deserialize\r\n    output = self._deserialize(value, attr, data)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/fields.py\", line 465, in _deserialize\r\n    data, errors = self.schema.load(value)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 588, in load\r\n    result, errors = self._do_load(data, many, partial=partial, postprocess=True)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 674, in _do_load\r\n    self._invoke_field_validators(unmarshal, data=result, many=many)\r\n  File \"/_/env/lib/python3.7/site-packages/marshmallow/schema.py\", line 894, in _invoke_field_validators\r\n    value = data[field_obj.attribute or field_name]\r\nTypeError: 'NoneType' object is not subscriptable\r\n```\n"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-1707",
        "base_commit": "40e9e978c170bdde4eeee1547729417665dbc34c",
        "problem_statement": "regression: iam.physical returns nan for aoi > 90\u00b0 when n = 1\n**Describe the bug**\r\nFor pvlib==0.9.5, when n = 1 (no reflection) and aoi > 90\u00b0, we get nan as result.\r\n\r\n**To Reproduce**\r\n```python\r\nimport pvlib\r\npvlib.iam.physical(aoi=100, n=1)\r\n```\r\nreturns `nan`.\r\n\r\n**Expected behavior**\r\nThe result should be `0`, as it was for pvlib <= 0.9.4.\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: '0.9.5'\r\n - ``pandas.__version__``:  '1.5.3'\r\n - python: 3.10.4\r\n\n"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-1072",
        "base_commit": "04a523fafbd61bc2e49420963b84ed8e2bd1b3cf",
        "problem_statement": "temperature.fuentes errors when given tz-aware inputs on pandas>=1.0.0\n**Describe the bug**\r\nWhen the weather timeseries inputs to `temperature.fuentes` have tz-aware index, an internal call to `np.diff(index)` returns an array of `Timedelta` objects instead of an array of nanosecond ints, throwing an error immediately after.  The error only happens when using pandas>=1.0.0; using 0.25.3 runs successfully, but emits the warning:\r\n\r\n```\r\n  /home/kevin/anaconda3/envs/pvlib-dev/lib/python3.7/site-packages/numpy/lib/function_base.py:1243: FutureWarning: Converting timezone-aware DatetimeArray to timezone-naive ndarray with 'datetime64[ns]' dtype. In the future, this will return an ndarray with 'object' dtype where each element is a 'pandas.Timestamp' with the correct 'tz'.\r\n  \tTo accept the future behavior, pass 'dtype=object'.\r\n  \tTo keep the old behavior, pass 'dtype=\"datetime64[ns]\"'.\r\n    a = asanyarray(a)\r\n```\r\n\r\n**To Reproduce**\r\n```python\r\nIn [1]: import pvlib\r\n   ...: import pandas as pd\r\n   ...: \r\n   ...: index_naive = pd.date_range('2019-01-01', freq='h', periods=3)\r\n   ...: \r\n   ...: kwargs = {\r\n   ...:     'poa_global': pd.Series(1000, index_naive),\r\n   ...:     'temp_air': pd.Series(20, index_naive),\r\n   ...:     'wind_speed': pd.Series(1, index_naive),\r\n   ...:     'noct_installed': 45\r\n   ...: }\r\n   ...: \r\n\r\nIn [2]: print(pvlib.temperature.fuentes(**kwargs))\r\n2019-01-01 00:00:00    47.85\r\n2019-01-01 01:00:00    50.85\r\n2019-01-01 02:00:00    50.85\r\nFreq: H, Name: tmod, dtype: float64\r\n\r\nIn [3]: kwargs['poa_global'].index = index_naive.tz_localize('UTC')\r\n   ...: print(pvlib.temperature.fuentes(**kwargs))\r\n   ...: \r\nTraceback (most recent call last):\r\n\r\n  File \"<ipython-input-3-ff99badadc91>\", line 2, in <module>\r\n    print(pvlib.temperature.fuentes(**kwargs))\r\n\r\n  File \"/home/kevin/anaconda3/lib/python3.7/site-packages/pvlib/temperature.py\", line 602, in fuentes\r\n    timedelta_hours = np.diff(poa_global.index).astype(float) / 1e9 / 60 / 60\r\n\r\nTypeError: float() argument must be a string or a number, not 'Timedelta'\r\n```\r\n\r\n**Expected behavior**\r\n`temperature.fuentes` should work with both tz-naive and tz-aware inputs.\r\n\r\n\r\n**Versions:**\r\n - ``pvlib.__version__``: 0.8.0\r\n - ``pandas.__version__``: 1.0.0+\r\n - python: 3.7.4 (default, Aug 13 2019, 20:35:49) \\n[GCC 7.3.0]\r\n\r\n\n"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-1606",
        "base_commit": "c78b50f4337ecbe536a961336ca91a1176efc0e8",
        "problem_statement": "golden-section search fails when upper and lower bounds are equal\n**Describe the bug**\r\nI was using pvlib for sometime now and until now I was always passing a big dataframe containing readings of a long period. Because of some changes in our software architecture, I need to pass the weather readings as a single reading (a dataframe with only one row) and I noticed that for readings that GHI-DHI are zero pvlib fails to calculate the output and returns below error while the same code executes correctly with weather information that has non-zero GHI-DHI:\r\n```python\r\nimport os\r\nimport pathlib\r\nimport time\r\nimport json\r\nfrom datetime import datetime\r\nfrom time import mktime, gmtime\r\n\r\nimport pandas as pd\r\n\r\nfrom pvlib import pvsystem\r\nfrom pvlib import location as pvlocation\r\nfrom pvlib import modelchain\r\nfrom pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS as PARAMS # not used -- to remove\r\nfrom pvlib.bifacial.pvfactors import pvfactors_timeseries\r\nfrom pvlib.temperature import TEMPERATURE_MODEL_PARAMETERS\r\n\r\nclass PV:\r\n    def pv_transform_time(self, val):\r\n        # tt = gmtime(val / 1000)\r\n        tt = gmtime(val)\r\n        dd = datetime.fromtimestamp(mktime(tt))\r\n        timestamp = pd.Timestamp(dd)\r\n        return timestamp\r\n\r\n    def __init__(self, model: str, inverter: str, latitude: float, longitude: float, **kwargs):\r\n        # super().__init__(**kwargs)\r\n\r\n        temperature_model_parameters = TEMPERATURE_MODEL_PARAMETERS[\"sapm\"][\r\n            \"open_rack_glass_glass\"\r\n        ]\r\n        # Load the database of CEC module model parameters\r\n        modules = pvsystem.retrieve_sam(\"cecmod\")\r\n        # Load the database of CEC inverter model parameters\r\n        inverters = pvsystem.retrieve_sam(\"cecinverter\")\r\n\r\n\r\n        # A bare bone PV simulator\r\n\r\n        # Load the database of CEC module model parameters\r\n        modules = pvsystem.retrieve_sam('cecmod')\r\n        inverters = pvsystem.retrieve_sam('cecinverter')\r\n        module_parameters = modules[model]\r\n        inverter_parameters = inverters[inverter]\r\n\r\n        location = pvlocation.Location(latitude=latitude, longitude=longitude)\r\n        system = pvsystem.PVSystem(module_parameters=module_parameters, inverter_parameters=inverter_parameters, temperature_model_parameters=temperature_model_parameters)\r\n        self.modelchain = modelchain.ModelChain(system, location, aoi_model='no_loss', spectral_model=\"no_loss\")\r\n\r\n    def process(self, data):\r\n        weather = pd.read_json(data)\r\n        # print(f\"raw_weather: {weather}\")\r\n        weather.drop('time.1', axis=1, inplace=True)\r\n        weather['time'] = pd.to_datetime(weather['time']).map(datetime.timestamp) # --> this works for the new process_weather code and also the old weather file\r\n        weather[\"time\"] = weather[\"time\"].apply(self.pv_transform_time)\r\n        weather.index = weather[\"time\"]\r\n        # print(f\"weather: {weather}\")\r\n        # print(weather.dtypes)\r\n        # print(weather['ghi'][0])\r\n        # print(type(weather['ghi'][0]))\r\n\r\n        # simulate\r\n        self.modelchain.run_model(weather)\r\n        # print(self.modelchain.results.ac.to_frame().to_json())\r\n        print(self.modelchain.results.ac)\r\n\r\n\r\n# good data\r\ngood_data = \"{\\\"time\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"},\\\"ghi\\\":{\\\"12\\\":36},\\\"dhi\\\":{\\\"12\\\":36},\\\"dni\\\":{\\\"12\\\":0},\\\"Tamb\\\":{\\\"12\\\":8.0},\\\"WindVel\\\":{\\\"12\\\":5.0},\\\"WindDir\\\":{\\\"12\\\":270},\\\"time.1\\\":{\\\"12\\\":\\\"2010-01-01 13:30:00+00:00\\\"}}\"\r\n\r\n# data that causes error\r\ndata = \"{\\\"time\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"},\\\"ghi\\\":{\\\"4\\\":0},\\\"dhi\\\":{\\\"4\\\":0},\\\"dni\\\":{\\\"4\\\":0},\\\"Tamb\\\":{\\\"4\\\":8.0},\\\"WindVel\\\":{\\\"4\\\":4.0},\\\"WindDir\\\":{\\\"4\\\":240},\\\"time.1\\\":{\\\"4\\\":\\\"2010-01-01 05:30:00+00:00\\\"}}\"\r\np1 = PV(model=\"Trina_Solar_TSM_300DEG5C_07_II_\", inverter=\"ABB__MICRO_0_25_I_OUTD_US_208__208V_\", latitude=51.204483, longitude=5.265472)\r\np1.process(good_data)\r\nprint(\"=====\")\r\np1.process(data)\r\n```\r\nError:\r\n```log\r\n$ python3 ./tmp-pv.py \r\ntime\r\n2010-01-01 13:30:00    7.825527\r\ndtype: float64\r\n=====\r\n/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py:340: RuntimeWarning: divide by zero encountered in divide\r\n  np.trunc(np.log(atol / (df['VH'] - df['VL'])) / np.log(phim1)))\r\nTraceback (most recent call last):\r\n  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 88, in <module>\r\n    p1.process(data)\r\n  File \"/home/user/workspace/enorch/simulator/simulator_processor/src/pv/./tmp-pv.py\", line 75, in process\r\n    self.modelchain.run_model(weather)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1770, in run_model\r\n    self._run_from_effective_irrad(weather)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 1858, in _run_from_effective_irrad\r\n    self.dc_model()\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 790, in cec\r\n    return self._singlediode(self.system.calcparams_cec)\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/modelchain.py\", line 772, in _singlediode\r\n    self.results.dc = tuple(itertools.starmap(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 931, in singlediode\r\n    return singlediode(photocurrent, saturation_current,\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/pvsystem.py\", line 2826, in singlediode\r\n    out = _singlediode._lambertw(\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/singlediode.py\", line 651, in _lambertw\r\n    p_mp, v_mp = _golden_sect_DataFrame(params, 0., v_oc * 1.14,\r\n  File \"/home/user/.local/lib/python3.10/site-packages/pvlib/tools.py\", line 364, in _golden_sect_DataFrame\r\n    raise Exception(\"Iterations exceeded maximum. Check that func\",\r\nException: ('Iterations exceeded maximum. Check that func', ' is not NaN in (lower, upper)')\r\n```\r\n\r\nI have to mention that for now the workaround that I am using is to pass the weather data as a dataframe with two rows, the first row is a good weather data that pvlib can process and the second row is the incoming weather reading (I can also post that code if you want).\r\n\r\n**Expected behavior**\r\nPVlib should have consistent behavior and regardless of GHI-DHI readings.\r\n\r\n**Versions:**\r\n```python\r\n>>> import pvlib\r\n>>> import pandas\r\n>>> pvlib.__version__\r\n'0.9.1'\r\n>>> pandas.__version__\r\n'1.4.3'\r\n``` \r\n - python: 3.10.6\r\n- OS: Ubuntu 22.04.1 LTS\n"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-1854",
        "base_commit": "27a3a07ebc84b11014d3753e4923902adf9a38c0",
        "problem_statement": "PVSystem with single Array generates an error\n**Is your feature request related to a problem? Please describe.**\r\n\r\nWhen a PVSystem has a single Array, you can't assign just the Array instance when constructing the PVSystem.\r\n\r\n```\r\nmount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\narray = pvlib.pvsystem.Array(mount=mount)\r\npv = pvlib.pvsystem.PVSystem(arrays=array)\r\n\r\n---------------------------------------------------------------------------\r\nTypeError                                 Traceback (most recent call last)\r\n<ipython-input-13-f5424e3db16a> in <module>\r\n      3 mount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\n      4 array = pvlib.pvsystem.Array(mount=mount)\r\n----> 5 pv = pvlib.pvsystem.PVSystem(arrays=array)\r\n\r\n~\\anaconda3\\lib\\site-packages\\pvlib\\pvsystem.py in __init__(self, arrays, surface_tilt, surface_azimuth, albedo, surface_type, module, module_type, module_parameters, temperature_model_parameters, modules_per_string, strings_per_inverter, inverter, inverter_parameters, racking_model, losses_parameters, name)\r\n    251                 array_losses_parameters,\r\n    252             ),)\r\n--> 253         elif len(arrays) == 0:\r\n    254             raise ValueError(\"PVSystem must have at least one Array. \"\r\n    255                              \"If you want to create a PVSystem instance \"\r\n\r\nTypeError: object of type 'Array' has no len()\r\n\r\n```\r\n\r\nNot a bug per se, since the PVSystem docstring requests that `arrays` be iterable. Still, a bit inconvenient to have to do this\r\n\r\n```\r\nmount = pvlib.pvsystem.FixedMount(surface_tilt=35, surface_azimuth=180)\r\narray = pvlib.pvsystem.Array(mount=mount)\r\npv = pvlib.pvsystem.PVSystem(arrays=[array])\r\n```\r\n\r\n**Describe the solution you'd like**\r\nHandle `arrays=array` where `array` is an instance of `Array`\r\n\r\n**Describe alternatives you've considered**\r\nStatus quo - either make the single Array into a list, or use the PVSystem kwargs.\r\n\n"
    },
    {
        "repo": "pvlib/pvlib-python",
        "instance_id": "pvlib__pvlib-python-1154",
        "base_commit": "0b8f24c265d76320067a5ee908a57d475cd1bb24",
        "problem_statement": "pvlib.irradiance.reindl() model generates NaNs when GHI = 0\n**Describe the bug**\r\nThe reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \r\n\r\n**Expected behavior**\r\nThe reindl function should result in zero sky diffuse when GHI is zero.\r\n\r\n\npvlib.irradiance.reindl() model generates NaNs when GHI = 0\n**Describe the bug**\r\nThe reindl function should give zero sky diffuse when GHI is zero. Instead it generates NaN or Inf values due to \"term3\" having a quotient that divides by GHI.  \r\n\r\n**Expected behavior**\r\nThe reindl function should result in zero sky diffuse when GHI is zero.\r\n\r\n\n"
    },
    {
        "repo": "pylint-dev/astroid",
        "instance_id": "pylint-dev__astroid-1978",
        "base_commit": "0c9ab0fe56703fa83c73e514a1020d398d23fa7f",
        "problem_statement": "Deprecation warnings from numpy\n### Steps to reproduce\r\n\r\n1. Run pylint over the following test case:\r\n\r\n```\r\n\"\"\"Test case\"\"\"\r\n\r\nimport numpy as np\r\nvalue = np.random.seed(1234)\r\n```\r\n\r\n### Current behavior\r\n```\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n/home/bje/source/nemo/myenv/lib/python3.10/site-packages/astroid/raw_building.py:470: FutureWarning: In the future `np.long` will be defined as the corresponding NumPy scalar.  (This may have returned Python scalars in past versions.\r\n  getattr(sys.modules[modname], name)\r\n```\r\n\r\n### Expected behavior\r\nThere should be no future warnings.\r\n\r\n### python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\" output\r\n2.12.13\n"
    },
    {
        "repo": "pylint-dev/astroid",
        "instance_id": "pylint-dev__astroid-1333",
        "base_commit": "d2a5b3c7b1e203fec3c7ca73c30eb1785d3d4d0a",
        "problem_statement": "astroid 2.9.1 breaks pylint with missing __init__.py: F0010: error while code parsing: Unable to load file __init__.py\n### Steps to reproduce\r\n> Steps provided are for Windows 11, but initial problem found in Ubuntu 20.04\r\n\r\n> Update 2022-01-04: Corrected repro steps and added more environment details\r\n\r\n1. Set up simple repo with following structure (all files can be empty):\r\n```\r\nroot_dir/\r\n|--src/\r\n|----project/ # Notice the missing __init__.py\r\n|------file.py # It can be empty, but I added `import os` at the top\r\n|----__init__.py\r\n```\r\n2. Open a command prompt\r\n3. `cd root_dir`\r\n4. `python -m venv venv`\r\n5. `venv/Scripts/activate`\r\n6. `pip install pylint astroid==2.9.1` # I also repro'd on the latest, 2.9.2\r\n7. `pylint src/project` # Updated from `pylint src`\r\n8. Observe failure:\r\n```\r\nsrc\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:\r\n```\r\n\r\n### Current behavior\r\nFails with `src\\project\\__init__.py:1:0: F0010: error while code parsing: Unable to load file src\\project\\__init__.py:`\r\n\r\n### Expected behavior\r\nDoes not fail with error.\r\n> If you replace step 6 with `pip install pylint astroid==2.9.0`, you get no failure with an empty output - since no files have content\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n2.9.1\r\n\r\n`python 3.9.1`\r\n`pylint 2.12.2 `\r\n\r\n\r\n\r\nThis issue has been observed with astroid `2.9.1` and `2.9.2`\n"
    },
    {
        "repo": "pylint-dev/astroid",
        "instance_id": "pylint-dev__astroid-1196",
        "base_commit": "39c2a9805970ca57093d32bbaf0e6a63e05041d8",
        "problem_statement": "getitem does not infer the actual unpacked value\nWhen trying to call `Dict.getitem()` on a context where we have a dict unpacking of anything beside a real dict, astroid currently raises an `AttributeError: 'getitem'`, which has 2 problems:\r\n\r\n- The object might be a reference against something constant, this pattern is usually seen when we have different sets of dicts that extend each other, and all of their values are inferrable. \r\n- We can have something that is uninferable, but in that case instead of an `AttributeError` I think it makes sense to raise the usual `AstroidIndexError` which is supposed to be already handled by the downstream.\r\n\r\n\r\nHere is a short reproducer;\r\n\r\n```py\r\nfrom astroid import parse\r\n\r\n\r\nsource = \"\"\"\r\nX = {\r\n    'A': 'B'\r\n}\r\n\r\nY = {\r\n    **X\r\n}\r\n\r\nKEY = 'A'\r\n\"\"\"\r\n\r\ntree = parse(source)\r\n\r\nfirst_dict = tree.body[0].value\r\nsecond_dict = tree.body[1].value\r\nkey = tree.body[2].value\r\n\r\nprint(f'{first_dict.getitem(key).value = }')\r\nprint(f'{second_dict.getitem(key).value = }')\r\n\r\n\r\n```\r\n\r\nThe current output;\r\n\r\n```\r\n $ python t1.py                                                                                                 3ms\r\nfirst_dict.getitem(key).value = 'B'\r\nTraceback (most recent call last):\r\n  File \"/home/isidentical/projects/astroid/t1.py\", line 23, in <module>\r\n    print(f'{second_dict.getitem(key).value = }')\r\n  File \"/home/isidentical/projects/astroid/astroid/nodes/node_classes.py\", line 2254, in getitem\r\n    return value.getitem(index, context)\r\nAttributeError: 'Name' object has no attribute 'getitem'\r\n```\r\n\r\nExpeceted output;\r\n```\r\n $ python t1.py                                                                                                 4ms\r\nfirst_dict.getitem(key).value = 'B'\r\nsecond_dict.getitem(key).value = 'B'\r\n\r\n```\r\n\n"
    },
    {
        "repo": "pylint-dev/astroid",
        "instance_id": "pylint-dev__astroid-1866",
        "base_commit": "6cf238d089cf4b6753c94cfc089b4a47487711e5",
        "problem_statement": "\"TypeError: unsupported format string passed to NoneType.__format__\" while running type inference in version 2.12.x\n### Steps to reproduce\r\n\r\nI have no concise reproducer. Exception happens every time I run pylint on some internal code, with astroid 2.12.10 and 2.12.12 (debian bookworm). It does _not_ happen with earlier versions of astroid (not with version 2.9). The pylinted code itself is \"valid\", it runs in production here.\r\n\r\n### Current behavior\r\n\r\nWhen running pylint on some code, I get this exception:\r\n```\r\nTraceback (most recent call last):\r\n  File \"/usr/lib/python3/dist-packages/pylint/utils/ast_walker.py\", line 90, in walk\r\n    callback(astroid)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 183, in visit_functiondef\r\n    inferred = _safe_infer_call_result(node, node)\r\n  File \"/usr/lib/python3/dist-packages/pylint/checkers/classes/special_methods_checker.py\", line 42, in _safe_infer_call_result\r\n    value = next(inferit)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/scoped_nodes/scoped_nodes.py\", line 1749, in infer_call_result\r\n    yield from returnnode.value.infer(context)\r\n  File \"/usr/lib/python3/dist-packages/astroid/nodes/node_ng.py\", line 159, in infer\r\n    results = list(self._explicit_inference(self, context, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/inference_tip.py\", line 45, in _inference_tip_cached\r\n    result = _cache[func, node] = list(func(*args, **kwargs))\r\n  File \"/usr/lib/python3/dist-packages/astroid/brain/brain_builtin_inference.py\", line 956, in _infer_str_format_call\r\n    formatted_string = format_template.format(*pos_values, **keyword_values)\r\nTypeError: unsupported format string passed to NoneType.__format__\r\n```\r\n\r\n### Expected behavior\r\n\r\nTypeError exception should not happen\r\n\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.12.10,\r\n2.12.12\n"
    },
    {
        "repo": "pylint-dev/astroid",
        "instance_id": "pylint-dev__astroid-1268",
        "base_commit": "ce5cbce5ba11cdc2f8139ade66feea1e181a7944",
        "problem_statement": "'AsStringVisitor' object has no attribute 'visit_unknown'\n```python\r\n>>> import astroid\r\n>>> astroid.nodes.Unknown().as_string()\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 609, in as_string\r\n    return AsStringVisitor()(self)\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/as_string.py\", line 56, in __call__\r\n    return node.accept(self).replace(DOC_NEWLINE, \"\\n\")\r\n  File \"/Users/tusharsadhwani/code/marvin-python/venv/lib/python3.9/site-packages/astroid/nodes/node_ng.py\", line 220, in accept\r\n    func = getattr(visitor, \"visit_\" + self.__class__.__name__.lower())\r\nAttributeError: 'AsStringVisitor' object has no attribute 'visit_unknown'\r\n>>> \r\n```\r\n### `python -c \"from astroid import __pkginfo__; print(__pkginfo__.version)\"` output\r\n\r\n2.8.6-dev0\n"
    },
    {
        "repo": "pyvista/pyvista",
        "instance_id": "pyvista__pyvista-4315",
        "base_commit": "db6ee8dd4a747b8864caae36c5d05883976a3ae5",
        "problem_statement": "Rectilinear grid does not allow Sequences as inputs\n### Describe the bug, what's wrong, and what you expected.\r\n\r\nRectilinear grid gives an error when `Sequence`s are passed in, but `ndarray` are ok.\r\n\r\n### Steps to reproduce the bug.\r\n\r\nThis doesn't work\r\n```python\r\nimport pyvista as pv\r\npv.RectilinearGrid([0, 1], [0, 1], [0, 1])\r\n```\r\n\r\nThis works\r\n```py\r\nimport pyvista as pv\r\nimport numpy as np\r\npv.RectilinearGrid(np.ndarray([0, 1]), np.ndarray([0, 1]), np.ndarray([0, 1]))\r\n```\r\n### System Information\r\n\r\n```shell\r\n--------------------------------------------------------------------------------\r\n  Date: Wed Apr 19 20:15:10 2023 UTC\r\n\r\n                OS : Linux\r\n            CPU(s) : 2\r\n           Machine : x86_64\r\n      Architecture : 64bit\r\n       Environment : IPython\r\n        GPU Vendor : Mesa/X.org\r\n      GPU Renderer : llvmpipe (LLVM 11.0.1, 256 bits)\r\n       GPU Version : 4.5 (Core Profile) Mesa 20.3.5\r\n\r\n  Python 3.11.2 (main, Mar 23 2023, 17:12:29) [GCC 10.2.1 20210110]\r\n\r\n           pyvista : 0.38.5\r\n               vtk : 9.2.6\r\n             numpy : 1.24.2\r\n           imageio : 2.27.0\r\n            scooby : 0.7.1\r\n             pooch : v1.7.0\r\n        matplotlib : 3.7.1\r\n           IPython : 8.12.0\r\n--------------------------------------------------------------------------------\r\n```\r\n\r\n\r\n### Screenshots\r\n\r\n_No response_\n"
    },
    {
        "repo": "pydicom/pydicom",
        "instance_id": "pydicom__pydicom-1694",
        "base_commit": "f8cf45b6c121e5a4bf4a43f71aba3bc64af3db9c",
        "problem_statement": "Dataset.to_json_dict can still generate exceptions when suppress_invalid_tags=True\n**Describe the bug**\r\nI'm using `Dataset.to_json_dict(suppress_invalid_tags=True)` and can live with losing invalid tags.  Unfortunately, I can still trigger an exception with something like  `2.0` in an `IS` field.\r\n\r\n**Expected behavior**\r\nto_json_dict shouldn't throw an error about an invalid tag when `suppress_invalid_tags` is enabled.\r\n\r\nMy thought was simply to move the `data_element = self[key]` into the try/catch block that's right after it.\r\n\r\n**Steps To Reproduce**\r\n\r\nTraceback:\r\n```\r\n  File \"dicom.py\", line 143, in create_dict\r\n    json_ds = ds.to_json_dict(suppress_invalid_tags=True)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataset.py\", line 2495, in to_json_dict\r\n    data_element = self[key]\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataset.py\", line 939, in __getitem__\r\n    self[tag] = DataElement_from_raw(elem, character_set, self)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/dataelem.py\", line 859, in DataElement_from_raw\r\n    value = convert_value(vr, raw, encoding)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/values.py\", line 771, in convert_value\r\n    return converter(byte_string, is_little_endian, num_format)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/values.py\", line 348, in convert_IS_string\r\n    return MultiString(num_string, valtype=pydicom.valuerep.IS)\r\n  File \"/usr/lib/python3/dist-packages/pydicom/valuerep.py\", line 1213, in MultiString\r\n    return valtype(splitup[0])\r\n  File \"/usr/lib/python3/dist-packages/pydicom/valuerep.py\", line 1131, in __new__\r\n    raise TypeError(\"Could not convert value to integer without loss\")\r\nTypeError: Could not convert value to integer without loss\r\n```\r\n\r\n**Your environment**\r\npython 3.7, pydicom 2.3\r\n\r\n\n"
    },
    {
        "repo": "pydicom/pydicom",
        "instance_id": "pydicom__pydicom-1413",
        "base_commit": "f909c76e31f759246cec3708dadd173c5d6e84b1",
        "problem_statement": "Error : a bytes-like object is required, not 'MultiValue'\nHello,\r\n\r\nI am getting following error while updating the tag LongTrianglePointIndexList (0066,0040),\r\n**TypeError: a bytes-like object is required, not 'MultiValue'**\r\n\r\nI noticed that the error  gets produced only when the VR is given as \"OL\" , works fine with \"OB\", \"OF\" etc.\r\n\r\nsample code (assume 'lineSeq' is the dicom dataset sequence):\r\n```python\r\nimport pydicom\r\nimport array\r\ndata=list(range(1,10))\r\ndata=array.array('H', indexData).tostring()  # to convert to unsigned short\r\nlineSeq.add_new(0x00660040, 'OL', data)   \r\nds.save_as(\"mydicom\")\r\n```\r\noutcome: **TypeError: a bytes-like object is required, not 'MultiValue'**\r\n\r\nusing version - 2.0.0.0\r\n\r\nAny help is appreciated.\r\n\r\nThank you\n"
    },
    {
        "repo": "pydicom/pydicom",
        "instance_id": "pydicom__pydicom-901",
        "base_commit": "3746878d8edf1cbda6fbcf35eec69f9ba79301ca",
        "problem_statement": "pydicom should not define handler, formatter and log level.\nThe `config` module (imported when pydicom is imported) defines a handler and set the log level for the pydicom logger. This should not be the case IMO. It should be the responsibility of the client code of pydicom to configure the logging module to its convenience. Otherwise one end up having multiple logs record as soon as pydicom is imported:\r\n\r\nExample:\r\n```\r\nCould not import pillow\r\n2018-03-25 15:27:29,744 :: DEBUG :: pydicom \r\n  Could not import pillow\r\nCould not import jpeg_ls\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import jpeg_ls\r\nCould not import gdcm\r\n2018-03-25 15:27:29,745 :: DEBUG :: pydicom \r\n  Could not import gdcm\r\n``` \r\nOr am I missing something?\n"
    },
    {
        "repo": "pydicom/pydicom",
        "instance_id": "pydicom__pydicom-1139",
        "base_commit": "b9fb05c177b685bf683f7f57b2d57374eb7d882d",
        "problem_statement": "Make PersonName3 iterable\n```python\r\nfrom pydicom import Dataset\r\n\r\nds = Dataset()\r\nds.PatientName = 'SomeName'\r\n\r\n'S' in ds.PatientName\r\n```\r\n```\r\nTraceback (most recent call last):\r\n  File \"<stdin>\", line 1, in <module>\r\nTypeError: argument of type 'PersonName3' is not iterable\r\n```\r\n\r\nI'm not really sure if this is intentional or if PN elements should support `str` methods. And yes I know I can `str(ds.PatientName)` but it's a bit silly, especially when I keep having to write exceptions to my element iterators just for PN elements.\n"
    },
    {
        "repo": "pydicom/pydicom",
        "instance_id": "pydicom__pydicom-1256",
        "base_commit": "49a3da4a3d9c24d7e8427a25048a1c7d5c4f7724",
        "problem_statement": "from_json does not correctly convert BulkDataURI's in SQ data elements\n**Describe the bug**\r\nWhen a DICOM object contains large data elements in SQ elements and is converted to JSON, those elements are correctly turned into BulkDataURI's. However, when the JSON is converted back to DICOM using from_json, the BulkDataURI's in SQ data elements are not converted back and warnings are thrown.\r\n\r\n**Expected behavior**\r\nThe BulkDataURI's in SQ data elements get converted back correctly.\r\n\r\n**Steps To Reproduce**\r\nTake the `waveform_ecg.dcm` in the test data, convert it to JSON, and then convert the JSON to DICOM\r\n\r\n**Your environment**\r\nmodule       | version\r\n------       | -------\r\nplatform     | macOS-10.15.7-x86_64-i386-64bit\r\nPython       | 3.8.2 (v3.8.2:7b3ab5921f, Feb 24 2020, 17:52:18)  [Clang 6.0 (clang-600.0.57)]\r\npydicom      | 2.1.0\r\ngdcm         | _module not found_\r\njpeg_ls      | _module not found_\r\nnumpy        | _module not found_\r\nPIL          | _module not found_\r\n\r\nThe problem is in `jsonrep.py` at line 227. I plan on submitting a pull-request today for this.\n"
    }
]